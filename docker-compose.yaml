

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD:  ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - airflow

  pgadmin:
    image: dpage/pgadmin4
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"  # Access pgAdmin via http://localhost:5050
    networks:
      - airflow
    depends_on:
      - postgres
    volumes:
      - pgadmin-data:/var/lib/pgadmin  # Store pgAdmin data

  airflow-init:
    image: apache/airflow:2.10.5-python3.12
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
    entrypoint: /bin/bash
    command:
      - -c
      - |
        pip install -r /requirements.txt && \
        airflow db init && \
        airflow users create --username admin --firstname admin --lastname user --role Admin --email admin@example.com --password admin
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./requirements.txt:/requirements.txt
      - ./.env:/opt/airflow/.env  # Ensure the .env file is correctly mounted
    networks:
      - airflow

  webserver:
    image: apache/airflow:2.10.5-python3.12
    depends_on:
      - postgres
    ports:
      - "8080:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: Asia/Kolkata
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}  # Reference from .env file
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}  # Reference from .env file
      AWS_S3_BUCKET: ${AWS_S3_BUCKET}  # Reference from .env file
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./requirements.txt:/requirements.txt
      - ./.env:/opt/airflow/.env  # Mount the .env file
    command: >
      bash -c "pip install -r /requirements.txt && airflow webserver"
    networks:
      - airflow

  scheduler:
    image: apache/airflow:2.10.5-python3.12
    depends_on:
      - webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}  # Reference from .env file
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}  # Reference from .env file
      AWS_S3_BUCKET: ${AWS_S3_BUCKET}  # Reference from .env file
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./requirements.txt:/requirements.txt
      - ./.env:/opt/airflow/.env  # Mount the .env file
    command: >
      bash -c "pip install -r /requirements.txt && airflow scheduler"
    networks:
      - airflow

volumes:
  postgres-db-volume:
  pgadmin-data:

networks:
  airflow:
