
# ğŸ“° News ETL Pipeline Project

This is an end-to-end ETL (Extract, Transform, Load) data pipeline that:
- **Fetches news data** from the News API
- **Stores raw data** in AWS S3 (or local storage)
- **Cleans/transforms the data** using Databricks
- **Loads cleaned data** into PostgreSQL
- **Orchestrates** all steps using Apache Airflow

---

## ğŸš€ Tech Stack

| Layer          | Technology Used             |
|----------------|-----------------------------|
| Orchestration  | Apache Airflow (Dockerized) |
| Ingestion      | News API + Python           |
| Storage        | AWS S3                      |
| Processing     | Databricks (PySpark)        |
| Load           | PostgreSQL                  |
| Logging        | Python `logging` module     |
| Containerization | Docker + Docker Compose   |

---

## ğŸ“ Folder Structure

```
news_etl_pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ news_ingestion_dag.py        # Airflow DAG
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cleaned_data/                # Output from Databricks
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ fetch_news.py                # News API data fetcher
â”‚   â”œâ”€â”€ upload_to_s3.py              # Uploads raw JSON to S3
â”‚   â”œâ”€â”€ download_from_s3.py          # Downloads for cleaning
â”‚   â””â”€â”€ load_news_to_postgres.py     # Loads cleaned CSV to Postgres
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/news_etl_pipeline.git
cd news_etl_pipeline
```

### 2. Setup Environment Variables

Rename `.env.example` to `.env` and set your credentials:

```env
POSTGRES_USER=your_user
POSTGRES_PASSWORD=your_password
POSTGRES_DB=your_db
AWS_ACCESS_KEY_ID=your_aws_key
AWS_SECRET_ACCESS_KEY=your_aws_secret
AWS_S3_BUCKET=news-data-bucket
```

### 3. Start Dockerized Services

```bash
docker compose up --build
```

This starts:
- Postgres
- pgAdmin (http://localhost:5050)
- Airflow Web UI (http://localhost:8080)

---

## ğŸ” Airflow Pipeline

The DAG runs the following steps:

1. **fetch_news** â€“ Fetches JSON data from News API
2. **upload_to_s3** â€“ Uploads raw data to S3
3. **download_from_s3** â€“ Downloads data for processing
4. **clean_and_save (Databricks)** â€“ Spark job cleans and saves as CSV
5. **load_to_postgres** â€“ Loads cleaned data into PostgreSQL

You can trigger the DAG from the Airflow UI or manually for testing.

---

## ğŸ§ª Sample Test Run

After starting all services:
- Place a CSV into `data/cleaned_data/` or use Databricks output
- Run the DAG
- Check `news_cleaned` table in PostgreSQL using pgAdmin

---

## ğŸ“Š Reporting Ideas (Optional)

You can add:
- Power BI / Tableau dashboards reading from PostgreSQL
- Email summary reports using Airflow email operator

